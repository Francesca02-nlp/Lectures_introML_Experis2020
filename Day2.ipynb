{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGgWxQ2oYc0AS4RWwMyRah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoalberti/Lectures_machinelearningbasics_nomath/blob/master/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEnwnRhNRRyx"
      },
      "source": [
        "# **Welcome Again!**\n",
        "\n",
        "## Introduction to Machine Learning Pt.2\n",
        "\n",
        "\n",
        "\n",
        "## **Lecturer :** Matteo Alberti\n",
        "\n",
        "![](data:image/jpeg;base64,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)\n",
        "\n",
        "*Contacts :* https://www.linkedin.com/in/matteo-alberti-170493/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sibE43BKRVLV"
      },
      "source": [
        "## Introduction to Supervised and Unsupervised Machine Learning\n",
        "\n",
        "![](https://www.diegocalvo.es/wp-content/uploads/2018/09/machine-learning-classification.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VraWNaq_RdBU"
      },
      "source": [
        "## What if ..\n",
        "\n",
        "$y=f(x)$\n",
        "\n",
        "### y is not a quantitative variable?\n",
        "\n",
        "![](https://whataftercollege.com/wp-content/uploads/2020/05/Classification-of-Machine-Learning.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HVXpqQxRs9D"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. The logistic function is defined as:\n",
        "\n",
        "$$logistic(η) = \\frac{1}{1+ exp(-η)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIt0TRdHnjCz"
      },
      "source": [
        "![](https://www.graphpad.com/guides/prism/8/curve-fitting/images/hmfile_hash_38a8acae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa9iLkBSnp7h"
      },
      "source": [
        " In the linear regression model, we have modelled the relationship between outcome and features with a linear equation:\n",
        "\n",
        " $yhat = alfa + beta_1 * x_1 + beta_2 * x_2 + … + beta_p * x_p + error$\n",
        "\n",
        "\n",
        " For classification, we prefer probabilities between 0 and 1\n",
        "\n",
        " $$logistic(η) = \\frac{1}{1+ exp(-(alfa + beta_1 * x_1 + beta_2 * x_2 + … + beta_p * x_p + error) )}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOSCehzwoqUp"
      },
      "source": [
        "# Metrics for binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zya01K73p5ho"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Confusion Matrix is a tool to determine the performance of classifier. It contains information about actual and predicted classifications. The below table shows confusion matrix of two-class, spam and non-spam classifier.\n",
        "\n",
        "![](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elyC9h5aqs__"
      },
      "source": [
        "### We can develop several metrics based on CM\n",
        "\n",
        "**Sensitivity** is also referred as True Positive Rate or Recall. It is measure of positive examples labeled as positive by classifier. It should be higher. For instance, proportion of emails which are spam among all spam emails.  \n",
        "\n",
        "$$TP/(TP+FP)$$\n",
        "\n",
        "*Example* : SPAM detection, Breast Cancer detection, ect ect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXm-5GvrglB"
      },
      "source": [
        "**Specificity** is also know as True Negative Rate. It is measure of negative examples labeled as negative by classifier. \n",
        "\n",
        "$$TN/(TN+FN)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ref1_hgr4Jw"
      },
      "source": [
        "**Precision** is ratio of total number of correctly classified positive examples and the total number of predicted positive examples\n",
        "\n",
        "\n",
        "$$TP/(TP+FN)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH5Qc322sJOW"
      },
      "source": [
        "**Accuracy** \n",
        "\n",
        "$$(TP+TN)/(ALL)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qJxjaH7qVWq"
      },
      "source": [
        "## Summary \n",
        "\n",
        "![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hermyPaJscB6"
      },
      "source": [
        "**F1 score** : weighted average of the recall (sensitivity) and precision.\n",
        "\n",
        "$$F1_{score} = 2* \\frac{Precision * Recall}{Precision + Recall}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfLY36nRtANp"
      },
      "source": [
        "## ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHMvCH9tDfT"
      },
      "source": [
        "**An ROC curve (or receiver operating characteristic curve) is a plot that summarizes the performance of a binary classification model on the positive class.**\n",
        "\n",
        "*AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.*\n",
        "\n",
        "The x-axis : **FPr** \n",
        "\n",
        "The y-axis : **TPr**\n",
        "\n",
        "ROC Curve: Plot of False Positive Rate (x) vs. True Positive Rate (y).\n",
        "\n",
        "The **true positive rate** is a fraction calculated as the total number of true positive predictions divided by the sum of the true positives and the false negatives (e.g. all examples in the positive class). The true positive rate is referred to as the **sensitivity** or the **recall**.\n",
        "\n",
        "**TruePositiveRate = TruePositives / (TruePositives + False Negatives)**\n",
        "\n",
        "The **false positive** rate is calculated as the total number of false positive predictions divided by the sum of the false positives and true negatives (e.g. all examples in the negative class).\n",
        "\n",
        "**FalsePositiveRate = FalsePositives / (FalsePositives + TrueNegatives)**\n",
        "\n",
        "We can think of the plot as the fraction of correct predictions for the positive class (y-axis) versus the fraction of errors for the negative class (x-axis).\n",
        "\n",
        "![](https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/36cdb4ec-0c7d-48cb-9a4d-7cb463f8b7c3/gr1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKCba-QNorT-"
      },
      "source": [
        "# Imbalanced Classification\n",
        "\n",
        "![](https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/QFE3K6B2QE4I5FJDTVOVGC7M5M.png)\n",
        "\n",
        "\n",
        "\n",
        "For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “no change” or “negative test result“), and the minority class is typically referred to as the positive outcome (e.g. “change” or “positive test result“)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnsMzkooVSv"
      },
      "source": [
        "# Multi Class Classification\n",
        "\n",
        " A classification task with more than two classes\n",
        "\n",
        " We have to adapt our *Logistic Function* to a new tasks . . . multi-class classification\n",
        "\n",
        " **Multinomial Logistic Regression** \n",
        "\n",
        "It uses **softmax function** instead of the sigmoid function the cross entropy loss function. **Softmax is a generalization of the sigmoid function.**\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/bdc1f8eaa8064d15893f1ba6426f20ff8e7149c5)\n",
        "\n",
        "\n",
        "*Steps :*\n",
        "\n",
        "- Raise e (the mathematical constant) to the power of each of those numbers.\n",
        "- Sum up all the exponentials (powers of ee). This result is the denominator.\n",
        "- Use each number’s exponential as its numerator.\n",
        "- $Probability=\\frac{Numerator}{Denominator}$\n",
        "\n",
        "\n",
        "The softmax function squashes all values to the range [0,1] and the sum of the elements is 1.\n",
        "\n",
        "\n",
        "*Example* : \n",
        "\n",
        "Given : -1, 0, 3, 5\n",
        "\n",
        "*We calculate Denominator :* $e^{-1}+e^{0} + e^{3} + e^{5}$ = 169.87\n",
        "\n",
        "*We calculate each Numerator $e^{x}$: * $e^{-1}=0.368, e^{}=1 . . . $\n",
        "\n",
        "*We calculate single probabilities $P(\\frac{$e^{x}{Denominator})$* : $0.368/169.87 = 0.002, 1/169.87= 0.006 . . $\n",
        "\n",
        "**Bigger x : bigger Probability**\n",
        "\n",
        "*Try to calculate all values probabilities and sum each other, what is the final number?*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlN5gTcleqjW"
      },
      "source": [
        "# Fuck paper and pen, use numpy!\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(xs):\n",
        "    return np.exp(xs) / sum(np.exp(xs))\n",
        "\n",
        "xs = np.array([-1, 0, 3, 5])\n",
        "print(softmax(xs))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB5Cg1KjquAT"
      },
      "source": [
        "## Confusion Matrix for multi-class classification\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/2228/1*yH2SM0DIUQlEiveK42NnBg.png)\n",
        "\n",
        "- TP = 7\n",
        "- TN = (2+3+2+1) = 8\n",
        "- FP = (8+9) = 17\n",
        "- FN = (1+3) = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6zzeRGOwl_w"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ-sP6xvwmJ5",
        "outputId": "b9e44b75-a7bb-4124-c3ed-991a799e3fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3tWxJL9weeb"
      },
      "source": [
        "### Wine quality Dataset\n",
        "\n",
        "- Fixed acidity\n",
        " -Volatile acidity\n",
        "- Citric acid\n",
        "- Residual sugar\n",
        "- Chlorides\n",
        "- Free sulfur dioxide\n",
        "- Total sulfur dioxide\n",
        "- Density\n",
        "- pH\n",
        "- Sulfates\n",
        "- Alcohol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGsrtJlmet-k"
      },
      "source": [
        "df_wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', delimiter=\";\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkJodmqzLVvg"
      },
      "source": [
        "# show dataset first 10 rows. TIPS : use .head()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp5W8byexYs3"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxb6aCnVMtUU"
      },
      "source": [
        "# how many classes has the target?\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWmOX3gQxS2s"
      },
      "source": [
        "# check df dimensions, how many rows? columns? TIPS : df.?\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "098kg_wCxEVX"
      },
      "source": [
        "# check how big is our dataset in KB/MB/GB\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkP69rRAO_gX"
      },
      "source": [
        "# missing values?\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KGyEwH1Pu10"
      },
      "source": [
        "# correlation matrix. DEFINE CORR\n",
        "\n",
        "corr = ###\n",
        "\n",
        "\n",
        "plt.subplots(figsize=(15,10))\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\n",
        "plt.show()\n",
        "\n",
        "# which variables are the most correlated?"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc7jc-omxY1F"
      },
      "source": [
        "# EXTRA : barplot of quality.  TIPS : use sns.countplot(dataset['name of the column target'])\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwcwl-lR9AO"
      },
      "source": [
        "# to classification.\n",
        "\n",
        "quality = wine[\"quality\"].values\n",
        "category = []\n",
        "for num in quality:\n",
        "    if num < 5:\n",
        "        category.append(\"Low\")\n",
        "    elif num > 6:\n",
        "        category.append(\"High\")\n",
        "    else:\n",
        "        category.append(\"Midium\")\n",
        "\n",
        "category = pd.DataFrame(data=category, columns=[\"category\"])\n",
        "data = pd.concat([wine, category], axis=1)\n",
        "data.drop(columns=\"quality\", axis=1, inplace=True)\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "\n",
        "# check again barplot OR PIEPLOT\n",
        "\n",
        "sns.countplot(y)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klrz-EE9fYDf"
      },
      "source": [
        "# Encoding. TIPS : fastest way? use function LabelEncoder()\n",
        "\n",
        "# - before you have to define as lab = LabelEncoder() than you can use the fit_transform method\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGo5EqgRSgeE",
        "outputId": "35beafc9-ae71-48a8-b82e-62fc745d4a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Data Splitting. TIPS : use train_test_split function\n",
        "\n",
        "# - 70/30\n",
        "# - add shuffle=True\n",
        "\n",
        "\n",
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3428, 11) (1470, 11) (3428,) (1470,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48k9V5Z8VTZZ",
        "outputId": "5bbb8444-7c94-4729-ddfc-5b15c8147631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "%%time\n",
        "log_regressor=LogisticRegression(solver='saga', multi_class='multinomial',max_iter=1000)\n",
        "\n",
        "# FIT\n",
        "\n",
        "log_regressor.fit(####)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.62 s, sys: 1.79 ms, total: 1.62 s\n",
            "Wall time: 1.63 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYPacrMqx5gs",
        "outputId": "f1dd0440-d1e8-403d-acbc-bbb258239b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Score!\n",
        "\n",
        "score=log_regressor.score(####)\n",
        "print('accuracy = {}'.format(score))\n",
        "\n",
        "# Write the accuracy formula TP, TN, ect ect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.7693877551020408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF38wW5uXMmz"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "y_pred1 = log_regressor.predict(####)\n",
        "print(classification_report(Y_test, y_pred1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl2QawYlhtNF"
      },
      "source": [
        "# features importance\n",
        "\n",
        "importance = regressor.coef_\n",
        "\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance[0]):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance[0]))], importance[0])\n",
        "plt.show()\n",
        "\n",
        "print(df_wine.columns)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VviUd6d-xcPV"
      },
      "source": [
        "# STOP!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeWZnKDNWjlz"
      },
      "source": [
        "# Decision Tree \n",
        "\n",
        "Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/02/Example-Decision-Tree.png)\n",
        "\n",
        " Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbrcaEGoyGS8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "DT_class = DecisionTreeClassifier(random_state=1) #unecessary\n",
        "\n",
        "DT_class.fit(###)\n",
        "\n",
        "y_pred1 = DT_class.predict(###)\n",
        "print(classification_report(Y_test, y_pred1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmlGqEjaKMJ"
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "Random forests are an ensemble learning technique that builds off of decision trees.\n",
        "\n",
        "***Ensemble methods**, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.*\n",
        "\n",
        "![](https://www.researchgate.net/profile/Evaldas_Vaiciukynas/publication/301638643/figure/fig1/AS:355471899807744@1461762513154/Architecture-of-the-random-forest-model.png)\n",
        "\n",
        "We'll not discuss about :\n",
        "\n",
        "- Bagging\n",
        "- Boosting\n",
        "\n",
        "We'll say that genereally speaking we would reduce the variance of a decision tree\n",
        "\n",
        "- Random sub-set of data\n",
        "- Random sub-set of features with replacement\n",
        "\n",
        "#### PRO\n",
        "\n",
        "- Handles higher dimensionality data very well.\n",
        "- Handles missing values and maintains accuracy for missing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-9gahCTyGP3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF_class = RandomForestClassifier(random_state=1)\n",
        "\n",
        "RF_class.fit(X_train, Y_train)\n",
        "\n",
        "y_pred2 = RF_class.predict(X_test)\n",
        "print(classification_report(Y_test, y_pred2))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYQ5Rm5nhgcg"
      },
      "source": [
        "# Interested in the complex version with K-fold and grid search?\n",
        "\"\"\"\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = RandomForestClassifier(random_state=2018, oob_score=True)\n",
        "param_dist = {\"n_estimators\": [50, 100, 150, 200, 250],\n",
        "              'min_samples_leaf': [1, 2, 4]}\n",
        "rfc_gs = GridSearchCV(clf, param_grid=param_dist, scoring='accuracy', cv=5)\n",
        "rfc_gs.fit(X_train, Y_train)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdYfKabBbVF-"
      },
      "source": [
        "# XGBoost\n",
        "\n",
        "XGBoost provides a parallel tree boosting\n",
        "\n",
        "The two reasons to use XGBoost are also the two goals of the project:\n",
        "\n",
        "- Execution Speed.\n",
        "- Model Performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoXE6G6dyLEj"
      },
      "source": [
        "import xgboost as xgb\n",
        "XGB_class = xgb.XGBClassifier(random_state=1)\n",
        "\n",
        "XGB_class.fit(X_train, Y_train)\n",
        "\n",
        "y_pred5 = XGB_class.predict(X_test)\n",
        "print(classification_report(Y_test, y_pred5))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my34dVfyuoQY"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
        "\n",
        "\n",
        "## Clustering vs Classification\n",
        "Before starting our discussion on k-means clustering, I would like point out the difference between clustering and classification.\n",
        "\n",
        "- Samples in a classification task have labels. Each data point is classified according to some measurements. Classification algorithms try to model the relationship between measurements (features) on samples and their assigned class. Then the model predicts the class of new samples.\n",
        "\n",
        "- Samples in clustering do not have labels. We expect the model to find structures in the data set so that similar samples can be grouped into clusters. We basically ask the model to label samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGeb9aRMn34o"
      },
      "source": [
        "# KMeans\n",
        "\n",
        "K-means clustering aims to partition data into k clusters in a way that data points in the same cluster are **similar** (*determined by distance*) and data points in the different clusters are farther apart.\n",
        "\n",
        "- There are many methods to measure the distance. \n",
        "\n",
        "\n",
        "Euclidean distance for example comes from **minkowski distance**\n",
        "\n",
        "![](https://rittikghosh.com/images/min.png)\n",
        "\n",
        "\n",
        "![](https://www.deeplearningitalia.com/wp-content/uploads/2018/12/ecuation-26.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YzaEKtXvRs5"
      },
      "source": [
        "### Let's try with syntetic data\n",
        "\n",
        "\n",
        "*K-means clustering tries to minimize distances within a cluster and maximize the distance between different clusters.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YNVUTJEvSKI"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples = 200, centers=4, cluster_std = 0.5, random_state = 0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjTQAZWmviUV",
        "outputId": "d8726e6f-23d7-4fed-ff7e-9ba9247cd4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# define + fit\n",
        "\n",
        "kmeans = KMeans(n_clusters = 4)\n",
        "kmeans.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhNpNUyCvy2P"
      },
      "source": [
        "**K-means is an iterative process.**\n",
        "\n",
        "After number of clusters are determined, it works by executing the following steps:\n",
        "\n",
        "- Randomly select centroids (center of cluster) for each cluster.\n",
        "- Calculate the distance of all data points to the centroids.\n",
        "- Assign data points to the closest cluster.\n",
        "- Find the new centroids of each cluster by taking the mean of all data points in the cluster.\n",
        "\n",
        "- Repeat steps 2,3 and 4 until all points converge and cluster centers stop moving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anb9WW0Kvmf9"
      },
      "source": [
        "y_pred = kmeans.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c = y_pred, s=50)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe70SFJNvQo5"
      },
      "source": [
        "## Back to Wine Dataset\n",
        "\n",
        "- How can I choose the right number of clusters?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYwrUc4bn7Vr"
      },
      "source": [
        "wcss = []\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "f3, ax = plt.subplots(figsize=(8, 6))\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ECE8F5oPDi",
        "outputId": "87805020-bd62-4ec3-95e7-c3728b3a4f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# build the model with the output from elbow method which is 2/3\n",
        "\n",
        "# Try yourself with 3\n",
        "\n",
        "clusterNum = 2\n",
        "k_means =KMeans(init='k-means++', n_clusters=clusterNum, n_init=12)\n",
        "k_means.fit(X)\n",
        "labels = k_means.labels_\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8dp5qV8oSnH"
      },
      "source": [
        "# We assign the labels to each row in dataframe.\n",
        "df_wine['Clus_km'] = labels\n",
        "print(df_wine.head())\n",
        "\n",
        "print(df_wine.drop('quality', axis=1).groupby('Clus_km').mean())\n",
        "\n",
        "# create 3 dimensional graph\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "f4 = plt.figure(1, figsize=(8, 6))\n",
        "plt.clf()\n",
        "ax = Axes3D(f4, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "\n",
        "plt.cla()\n",
        "ax.set_xlabel('alcohol')\n",
        "ax.set_ylabel('total sulfur dioxide')\n",
        "ax.set_zlabel('pH')\n",
        "\n",
        "ax.scatter(X[:, 9], X[:, 5], X[:, 7], c= labels.astype(np.float))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfw47WRqlouN"
      },
      "source": [
        "# Do you wanna extra exercises?\n",
        "\n",
        "### medium-strong\n",
        "- use random forest as regressor in Day1\n",
        "\n",
        "### light\n",
        "\n",
        "- analyze kmeans number of cluster how variate performances\n",
        "- analyze classifications performances with different values"
      ]
    }
  ]
}